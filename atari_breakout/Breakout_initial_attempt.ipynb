{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fec68a-c140-4cfe-88c6-91a454fcef62",
   "metadata": {},
   "source": [
    "# Atari Breakout in RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d76bd2-8a69-429c-bf5c-b0fca8cc9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ray[rllib]\n",
    "#!pip install ipywidgets gputil\n",
    "#!pip install gym[atari] autorom[accept-rom-license] ray[rllib] torch torchvision atari-py ale-py ale_py\n",
    "#!pip install \"gym[accept-rom-license, atari]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34c57b-d04e-443e-8145-24d99efa8c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 12:53:51,146\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-04-25 12:53:51,712\tINFO tune.py:624 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-25 13:06:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:12:38.18        </td></tr>\n",
       "<tr><td>Memory:      </td><td>23.6/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 20.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ALE_Breakout-v5_8a226_00000</td><td>RUNNING </td><td>192.168.1.114:580098</td><td style=\"text-align: right;\">  1164</td><td style=\"text-align: right;\">         741.705</td><td style=\"text-align: right;\">5820000</td><td style=\"text-align: right;\">     5.8</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            358.04</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=580262)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "\u001b[36m(RolloutWorker pid=580262)\u001b[0m [Powered by Stella]\n",
      "2024-04-25 12:54:04,982\tWARNING logger.py:186 -- Remote file not found: /home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/result.json\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Restored on 192.168.1.114 from checkpoint: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_11-19-27/PPO_ALE_Breakout-v5_59d6c_00000_0_2024-04-25_11-19-27/checkpoint_000058)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m 2024-04-25 12:54:11,525\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m [Powered by Stella]\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics  </th><th>counters                                                                                                                                    </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_timesteps_total</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                                          </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                   </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </th><th>timers                                                                                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ALE_Breakout-v5_8a226_00000</td><td style=\"text-align: right;\">                5820000</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 5820000, &#x27;num_env_steps_trained&#x27;: 5820000, &#x27;num_agent_steps_sampled&#x27;: 5820000, &#x27;num_agent_steps_trained&#x27;: 5820000}</td><td>{}              </td><td style=\"text-align: right;\">            358.04</td><td>{}             </td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">                  5.8</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">                     35804</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;custom_metrics&#x27;: {}, &#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 1.7877779172606833e-249, &#x27;cur_lr&#x27;: 5e-05, &#x27;total_loss&#x27;: 0.008561232667416334, &#x27;policy_loss&#x27;: -0.0036237090453505516, &#x27;vf_loss&#x27;: 0.020468183851335198, &#x27;vf_explained_var&#x27;: 0.9568745845556259, &#x27;kl&#x27;: 0.0039137388137169184, &#x27;entropy&#x27;: 0.8283239269256591, &#x27;entropy_coeff&#x27;: 0.009999999999999998}, &#x27;model&#x27;: {}, &#x27;num_grad_updates_lifetime&#x27;: 8350.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 49.5}}, &#x27;num_env_steps_sampled&#x27;: 5820000, &#x27;num_env_steps_trained&#x27;: 5820000, &#x27;num_agent_steps_sampled&#x27;: 5820000, &#x27;num_agent_steps_trained&#x27;: 5820000}</td><td style=\"text-align: right;\">                  5820000</td><td style=\"text-align: right;\">                  5820000</td><td style=\"text-align: right;\">                5820000</td><td style=\"text-align: right;\">                             5000</td><td style=\"text-align: right;\">                                   567.231</td><td style=\"text-align: right;\">                5820000</td><td style=\"text-align: right;\">                             5000</td><td style=\"text-align: right;\">                                   567.231</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   19</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         5000</td><td>{&#x27;cpu_util_percent&#x27;: 63.1, &#x27;ram_util_percent&#x27;: 19.08333333333334, &#x27;gpu_util_percent0&#x27;: 0.30333333333333334, &#x27;vram_util_percent0&#x27;: 0.389453125}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 10.207625369502928, &#x27;mean_inference_ms&#x27;: 53.537665356405114, &#x27;mean_action_processing_ms&#x27;: 0.6494907893233544, &#x27;mean_env_wait_ms&#x27;: 21.199948569174843, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 25.0, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 5.8, &#x27;episode_len_mean&#x27;: 358.04, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 15, &#x27;episodes_timesteps_total&#x27;: 35804, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [3.0, 4.0, 4.0, 2.0, 8.0, 0.0, 4.0, 0.0, 8.0, 6.0, 8.0, 0.0, 4.0, 8.0, 0.0, 0.0, 19.0, 4.0, 7.0, 11.0, 16.0, 0.0, 7.0, 4.0, 11.0, 5.0, 0.0, 7.0, 11.0, 7.0, 6.0, 4.0, 3.0, 8.0, 9.0, 11.0, 12.0, 6.0, 3.0, 8.0, 6.0, 12.0, 4.0, 9.0, 0.0, 0.0, 7.0, 2.0, 4.0, 0.0, 0.0, 7.0, 6.0, 13.0, 3.0, 9.0, 4.0, 3.0, 4.0, 5.0, 13.0, 0.0, 11.0, 4.0, 9.0, 7.0, 13.0, 11.0, 15.0, 0.0, 0.0, 11.0, 0.0, 7.0, 0.0, 6.0, 0.0, 25.0, 9.0, 8.0, 0.0, 8.0, 0.0, 7.0, 4.0, 4.0, 6.0, 4.0, 4.0, 13.0, 4.0, 4.0, 5.0, 4.0, 6.0, 7.0, 3.0, 8.0, 0.0, 4.0], &#x27;episode_lengths&#x27;: [277, 318, 315, 217, 456, 156, 304, 140, 465, 388, 487, 153, 295, 482, 139, 157, 725, 303, 423, 493, 677, 145, 443, 315, 484, 328, 133, 431, 491, 430, 398, 305, 269, 471, 493, 483, 617, 394, 284, 366, 385, 606, 306, 491, 147, 155, 417, 231, 316, 139, 148, 425, 398, 572, 293, 394, 307, 274, 308, 358, 562, 155, 482, 304, 518, 437, 569, 482, 517, 131, 130, 589, 134, 438, 141, 391, 135, 659, 513, 460, 147, 471, 144, 442, 295, 296, 374, 314, 321, 575, 345, 314, 363, 308, 393, 442, 277, 489, 136, 291]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 10.207625369502928, &#x27;mean_inference_ms&#x27;: 53.537665356405114, &#x27;mean_action_processing_ms&#x27;: 0.6494907893233544, &#x27;mean_env_wait_ms&#x27;: 21.199948569174843, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 8822.768, &#x27;restore_workers_time_ms&#x27;: 0.018, &#x27;training_step_time_ms&#x27;: 8822.72, &#x27;sample_time_ms&#x27;: 6101.054, &#x27;load_time_ms&#x27;: 101.822, &#x27;load_throughput&#x27;: 49105.2, &#x27;learn_time_ms&#x27;: 2589.073, &#x27;learn_throughput&#x27;: 1931.193, &#x27;synch_weights_time_ms&#x27;: 28.12}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 12:54:14,405\tWARNING logger.py:186 -- Remote file not found: /home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/progress.csv\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000001)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000002)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000003)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000004)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000005)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000006)\n",
      "\u001b[36m(PPO pid=580098)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_12-53-51/PPO_ALE_Breakout-v5_8a226_00000_0_2024-04-25_12-53-51/checkpoint_000007)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray import tune\n",
    "import ale_py\n",
    "\n",
    "\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(num_cpus=20, num_gpus=1)\n",
    "    #env = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "checkpoint_directory = '/home/fitz/ray_results/PPO_2024-04-25_11-19-27/PPO_ALE_Breakout-v5_59d6c_00000_0_2024-04-25_11-19-27/checkpoint_000058'\n",
    "config = {\n",
    "    \"env\": \"ALE/Breakout-v5\",\n",
    "    \"num_workers\": 19,\n",
    "    \"num_envs_per_worker\": 4,\n",
    "    \"num_gpus\": 1,\n",
    "    \"framework\": \"torch\",  \n",
    "    \"gamma\": 0.99,\n",
    "    \"train_batch_size\": 5000,\n",
    "    \"rollout_fragment_length\": \"auto\",\n",
    "    \"sgd_minibatch_size\": 500,\n",
    "    \"num_sgd_iter\": 10,\n",
    "    \"clip_param\": 0.1,\n",
    "    \"entropy_coeff\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "#config = {\n",
    "#        \"env\": \"ALE/Breakout-v5\",\n",
    "#        \"preprocessor_pref\": None,\n",
    "#        \"gamma\": 0.99,\n",
    "#        \"num_gpus\": 1,\n",
    "#        \"num_workers\": 19,\n",
    "#        \"num_envs_per_worker\": 4,\n",
    "#        \"create_env_on_driver\": True,\n",
    "#        \"lambda\": 0.95,\n",
    "#        \"kl_coeff\": 0.5,\n",
    "#        \"clip_rewards\": True,\n",
    "#        \"clip_param\": 0.1,\n",
    "#        \"vf_clip_param\": 10.0,\n",
    "#        \"entropy_coeff\": 0.01,\n",
    "#        \"rollout_fragment_length\": \"auto\",\n",
    "#        \"sgd_minibatch_size\": 500,\n",
    "#        \"num_sgd_iter\": 10,\n",
    "#        \"batch_mode\": \"truncate_episodes\",\n",
    "#        \"framework\": \"torch\",\n",
    "#}\n",
    "\n",
    "stop_criteria = {\n",
    "    \"training_iteration\": 2700,\n",
    "    \"episode_reward_mean\": 200\n",
    "}\n",
    "\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=10,\n",
    "    keep_checkpoints_num=10,  # Keep 10 checkpoints\n",
    "    checkpoint_score_attr=\"episode_reward_mean\",  # Retain checkpoints with the highest episode rewards\n",
    "    restore=checkpoint_directory\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c477aa-69b6-4401-82e5-3f9205cee8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_11-19-27/PPO_ALE_Breakout-v5_59d6c_00000_0_2024-04-25_11-19-27/checkpoint_000058)\n"
     ]
    }
   ],
   "source": [
    "best_trial = results.get_best_trial(\"episode_reward_mean\", \"max\", \"last\")\n",
    "best_checkpoint = results.get_best_checkpoint(best_trial, \"episode_reward_mean\", \"max\")\n",
    "print(\"Best checkpoint:\", best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c75591-821c-4243-af52-9bf8c80082bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae5c0c-3c9d-4abe-b2db-aae195118505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
