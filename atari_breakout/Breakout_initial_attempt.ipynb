{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fec68a-c140-4cfe-88c6-91a454fcef62",
   "metadata": {},
   "source": [
    "# Atari Breakout in RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d76bd2-8a69-429c-bf5c-b0fca8cc9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ray[rllib]\n",
    "#!pip install ipywidgets gputil\n",
    "#!pip install gym[atari] autorom[accept-rom-license] ray[rllib] torch torchvision atari-py ale-py ale_py\n",
    "#!pip install \"gym[accept-rom-license, atari]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de34c57b-d04e-443e-8145-24d99efa8c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 22:15:28,748\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-04-25 22:15:29,352\tINFO tune.py:624 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-25 23:16:04</td></tr>\n",
       "<tr><td>Running for: </td><td>01:00:35.26        </td></tr>\n",
       "<tr><td>Memory:      </td><td>27.1/125.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 20.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">      ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ALE_Breakout-v5_ff7e5_00000</td><td>RUNNING </td><td>192.168.1.114:1559396</td><td style=\"text-align: right;\">  4657</td><td style=\"text-align: right;\">         3609.82</td><td style=\"text-align: right;\">23285000</td><td style=\"text-align: right;\">    8.25</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            428.37</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=1559547)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "\u001b[36m(RolloutWorker pid=1559547)\u001b[0m [Powered by Stella]\n",
      "2024-04-25 22:15:42,872\tWARNING logger.py:186 -- Remote file not found: /home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/result.json\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Restored on 192.168.1.114 from checkpoint: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_21-36-24/PPO_ALE_Breakout-v5_89cd7_00000_0_2024-04-25_21-36-24/checkpoint_000024)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m 2024-04-25 22:15:49,286\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m [Powered by Stella]\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics  </th><th>counters                                                                                                                                        </th><th>custom_metrics  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_timesteps_total</th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                                   </th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                 </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </th><th>timers                                                                                                                                                                                                                                                                                  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ALE_Breakout-v5_ff7e5_00000</td><td style=\"text-align: right;\">               23285000</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 23285000, &#x27;num_env_steps_trained&#x27;: 23285000, &#x27;num_agent_steps_sampled&#x27;: 23285000, &#x27;num_agent_steps_trained&#x27;: 23285000}</td><td>{}              </td><td style=\"text-align: right;\">            428.37</td><td>{}             </td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">                 8.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">                     42837</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;custom_metrics&#x27;: {}, &#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.0, &#x27;cur_lr&#x27;: 5e-05, &#x27;total_loss&#x27;: 0.007331774532794952, &#x27;policy_loss&#x27;: -0.006946268081665039, &#x27;vf_loss&#x27;: 0.020627815504558385, &#x27;vf_explained_var&#x27;: 0.9706599086523056, &#x27;kl&#x27;: 0.0027288128978216262, &#x27;entropy&#x27;: 0.6349776422977448, &#x27;entropy_coeff&#x27;: 0.009999999999999998}, &#x27;model&#x27;: {}, &#x27;num_grad_updates_lifetime&#x27;: 40650.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 49.5}}, &#x27;num_env_steps_sampled&#x27;: 23285000, &#x27;num_env_steps_trained&#x27;: 23285000, &#x27;num_agent_steps_sampled&#x27;: 23285000, &#x27;num_agent_steps_trained&#x27;: 23285000}</td><td style=\"text-align: right;\">                 23285000</td><td style=\"text-align: right;\">                 23285000</td><td style=\"text-align: right;\">               23285000</td><td style=\"text-align: right;\">                             5000</td><td style=\"text-align: right;\">                                   557.144</td><td style=\"text-align: right;\">               23285000</td><td style=\"text-align: right;\">                             5000</td><td style=\"text-align: right;\">                                   557.144</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   19</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         5000</td><td>{&#x27;cpu_util_percent&#x27;: 64.9, &#x27;ram_util_percent&#x27;: 21.891666666666666, &#x27;gpu_util_percent0&#x27;: 0.32, &#x27;vram_util_percent0&#x27;: 0.3851562499999999}</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 10.461301448060778, &#x27;mean_inference_ms&#x27;: 53.65891144151877, &#x27;mean_action_processing_ms&#x27;: 0.6621673974926255, &#x27;mean_env_wait_ms&#x27;: 21.39927384988722, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 24.0, &#x27;episode_reward_min&#x27;: 0.0, &#x27;episode_reward_mean&#x27;: 8.25, &#x27;episode_len_mean&#x27;: 428.37, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 15, &#x27;episodes_timesteps_total&#x27;: 42837, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [5.0, 15.0, 16.0, 15.0, 5.0, 15.0, 9.0, 4.0, 13.0, 9.0, 13.0, 13.0, 10.0, 8.0, 10.0, 7.0, 7.0, 4.0, 2.0, 0.0, 0.0, 16.0, 9.0, 21.0, 5.0, 0.0, 6.0, 0.0, 7.0, 4.0, 5.0, 10.0, 5.0, 16.0, 10.0, 5.0, 10.0, 14.0, 11.0, 1.0, 8.0, 5.0, 8.0, 9.0, 9.0, 5.0, 10.0, 13.0, 5.0, 11.0, 9.0, 10.0, 8.0, 2.0, 9.0, 7.0, 9.0, 2.0, 7.0, 10.0, 5.0, 2.0, 8.0, 24.0, 9.0, 22.0, 9.0, 4.0, 5.0, 7.0, 9.0, 7.0, 7.0, 9.0, 2.0, 5.0, 11.0, 3.0, 14.0, 19.0, 7.0, 7.0, 5.0, 7.0, 11.0, 15.0, 10.0, 7.0, 11.0, 0.0, 7.0, 5.0, 7.0, 10.0, 9.0, 6.0, 14.0, 2.0, 4.0, 9.0], &#x27;episode_lengths&#x27;: [336, 628, 659, 589, 327, 625, 499, 305, 565, 488, 556, 514, 365, 446, 548, 435, 421, 294, 218, 158, 129, 636, 496, 716, 332, 129, 375, 157, 377, 317, 325, 508, 319, 680, 546, 328, 433, 591, 588, 200, 477, 322, 470, 485, 495, 324, 507, 514, 330, 493, 490, 506, 446, 216, 482, 388, 504, 203, 387, 447, 340, 224, 469, 764, 483, 687, 483, 293, 335, 419, 506, 375, 392, 505, 231, 318, 545, 269, 558, 680, 363, 371, 364, 409, 554, 622, 540, 368, 446, 150, 364, 324, 430, 546, 486, 367, 598, 232, 297, 486]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 10.461301448060778, &#x27;mean_inference_ms&#x27;: 53.65891144151877, &#x27;mean_action_processing_ms&#x27;: 0.6621673974926255, &#x27;mean_env_wait_ms&#x27;: 21.39927384988722, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td>{&#x27;training_iteration_time_ms&#x27;: 8919.423, &#x27;restore_workers_time_ms&#x27;: 0.02, &#x27;training_step_time_ms&#x27;: 8919.373, &#x27;sample_time_ms&#x27;: 6158.959, &#x27;load_time_ms&#x27;: 101.957, &#x27;load_throughput&#x27;: 49040.207, &#x27;learn_time_ms&#x27;: 2628.06, &#x27;learn_throughput&#x27;: 1902.544, &#x27;synch_weights_time_ms&#x27;: 27.668}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 22:15:52,158\tWARNING logger.py:186 -- Remote file not found: /home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/progress.csv\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000001)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000002)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000003)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000004)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000005)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000006)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000007)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000008)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000009)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000010)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000011)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000012)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000013)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000014)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000015)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000016)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000017)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000018)\n",
      "\u001b[36m(PPO pid=1559396)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000019)\n",
      "2024-04-25 23:16:04,541\tWARNING tune.py:229 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-04-25 23:16:04,616\tINFO tune.py:1021 -- Wrote the latest version of all result files and experiment state to '/home/fitz/ray_results/PPO_2024-04-25_22-15-29' in 0.0711s.\n",
      "2024-04-25 23:16:09,068\tINFO tune.py:1053 -- Total run time: 3639.72 seconds (3635.18 seconds for the tuning loop).\n",
      "2024-04-25 23:16:09,069\tWARNING tune.py:1068 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray import tune\n",
    "import ale_py\n",
    "\n",
    "\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(num_cpus=20, num_gpus=1)\n",
    "    #env = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "checkpoint_directory = '/home/fitz/ray_results/PPO_2024-04-25_21-36-24/PPO_ALE_Breakout-v5_89cd7_00000_0_2024-04-25_21-36-24/checkpoint_000024'\n",
    "config = {\n",
    "    \"env\": \"ALE/Breakout-v5\",\n",
    "    \"num_workers\": 19,\n",
    "    \"num_envs_per_worker\": 4,\n",
    "    \"num_gpus\": 1,\n",
    "    \"framework\": \"torch\",  \n",
    "    \"gamma\": 0.99,\n",
    "    \"train_batch_size\": 5000,\n",
    "    \"rollout_fragment_length\": \"auto\",\n",
    "    \"sgd_minibatch_size\": 500,\n",
    "    \"num_sgd_iter\": 10,\n",
    "    \"clip_param\": 0.1,\n",
    "    \"entropy_coeff\": 0.01,\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    \"lambda\": 0.95,\n",
    "    \"kl_coeff\": 0.5,\n",
    "    \"clip_rewards\": True,\n",
    "    \"clip_param\": 0.1,\n",
    "    \"vf_clip_param\": 10.0,\n",
    "    \"entropy_coeff\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "#config = {\n",
    "#        \"env\": \"ALE/Breakout-v5\",\n",
    "#        \"preprocessor_pref\": None,\n",
    "#   #     \"gamma\": 0.99,\n",
    "#  #      \"num_gpus\": 1,\n",
    "# #       \"num_workers\": 19,\n",
    "##        \"num_envs_per_worker\": 4,\n",
    "#        \"create_env_on_driver\": True,\n",
    "#        \"lambda\": 0.95,\n",
    "#        \"kl_coeff\": 0.5,\n",
    "#        \"clip_rewards\": True,\n",
    "#        \"clip_param\": 0.1,\n",
    "#        \"vf_clip_param\": 10.0,\n",
    "#        \"entropy_coeff\": 0.01,\n",
    "##        \"rollout_fragment_length\": \"auto\",\n",
    "##        \"sgd_minibatch_size\": 500,\n",
    "##        \"num_sgd_iter\": 10,\n",
    "##        \"batch_mode\": \"truncate_episodes\",\n",
    "##        \"framework\": \"torch\",\n",
    "#}\n",
    "\n",
    "stop_criteria = {\n",
    "    \"training_iteration\": 10000,\n",
    "    \"episode_reward_mean\": 200\n",
    "}\n",
    "\n",
    "results = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop=stop_criteria,\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=20,\n",
    "    keep_checkpoints_num=20,  # Keep 10 checkpoints\n",
    "    checkpoint_score_attr=\"episode_reward_mean\",  # Retain checkpoints with the highest episode rewards\n",
    "    restore=checkpoint_directory\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c477aa-69b6-4401-82e5-3f9205cee8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: Checkpoint(filesystem=local, path=/home/fitz/ray_results/PPO_2024-04-25_22-15-29/PPO_ALE_Breakout-v5_ff7e5_00000_0_2024-04-25_22-15-29/checkpoint_000012)\n"
     ]
    }
   ],
   "source": [
    "best_trial = results.get_best_trial(\"episode_reward_mean\", \"max\", \"last\")\n",
    "best_checkpoint = results.get_best_checkpoint(best_trial, \"episode_reward_mean\", \"max\")\n",
    "print(\"Best checkpoint:\", best_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c75591-821c-4243-af52-9bf8c80082bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae5c0c-3c9d-4abe-b2db-aae195118505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
